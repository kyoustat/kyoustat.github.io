% %!TEX encoding = UTF-8 Unicode
\documentclass[11pt]{article}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor} % usenames : 16 basic colors,  dvipsnames : other 68 colors, svgnames : another 150 colors. It is used instead of \usepackage{color}. Can find descriptions for color in xcolor package. 

\usepackage{enumitem} % for \begin{description}
% custom enumeration
% https://tex.stackexchange.com/questions/37740/enumerate-with-properties
\newlist{Properties}{enumerate}{2} 
\setlist[Properties]{label=\textbf{{Property \arabic*.}} ,itemindent=*}
\newlist{Facts}{enumerate}{2} 
\setlist[Facts]{label=\textbf{{Fact \arabic*}} ,itemindent=*}

\usepackage[table]{xcolor}
\usepackage{rotfloat} % For [H] option in sidewaystable
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{longtable} % table that spans multiple pages
\usepackage{authblk}
%\usepackage{kotex}
\usepackage{multirow}
\usepackage[colorlinks]{hyperref} % it is needed for todonotes package. 
\usepackage[colorinlistoftodos, textsize=scriptsize]{todonotes} % For making notes. disable option removes all notes. 
\usepackage{subcaption} % for \begin{subtable}
\usepackage{array}
%\newcolumntype{L}{>{\centering\arraybackslash}m{0.1\linewidth}}
\newcolumntype{R}{>{$}r<{$}}
\newcolumntype{L}{>{$}l<{$}}
\newcolumntype{M}{R@{${}={}$}L}

% https://tex.stackexchange.com/questions/163735/how-to-keep-fixed-height-of-mdframed-in-latex
% http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/tcolorbox/tcolorbox.pdf
\usepackage{tcolorbox}
\newtcbox{\mybox}[1][]{%
	nobeforeafter,
	colframe=gray,
	boxrule=0.5pt,
	minipage,
	width=0.95\linewidth, 
	valign=center,
	height=1.8cm,               %%% <--- change here
	arc=3pt,
	boxsep=0pt,
	#1
}
\newtcbox{\mybigbox}[1][]{%
	nobeforeafter,
	colframe=gray,
	boxrule=0.5pt,
	minipage,
	width=0.95\linewidth, 
	valign=center,
	height=3cm,               %%% <--- change here
	arc=3pt,
	boxsep=0pt,
	#1
}


\renewcommand{\baselinestretch}{1.25}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}

%\newenvironment{proof}[1][Proof]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{remark}[1][Remark]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}



%\newcommand{\qed}{\nobreak \ifvmode \relax \else
%      \ifdim\lastskip<1.5em \hskip-\lastskip
%      \hskip1.5em plus0em minus0.5em \fi \nobreak
%      \vrule height0.75em width0.5em depth0.25em\fi}

%\def\qed{\space$\Box$ \par \vspace{.15in}}

% ***************************************************************
% Changed by Kisung You
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\todokl}{\todo[inline,color=lime]}

\newcommand{\simiid}{\overset{iid}{\sim}}
\newcommand{\simind}{\overset{ind}{\sim}}

% arrows
\newcommand{\Lra}{\Longrightarrow}
\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}

% caligraph
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}} 
\newcommand{\calZ}{\mathcal{Z}}

% mathbb's
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}} 
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbX}{\mathbb{X}}

% mathbf's
\newcommand{\bfS}{\mathbf{S}}
\newcommand{\bfLambda}{\mathbf{\Lambda}}

% mathsf's
\newcommand{\sfD}{\mathsf{D}} % persistence diagram
\newcommand{\sfL}{\mathsf{L}} % persistence landscape
\newcommand{\sfC}{\mathsf{C}}   % Cech complex
\newcommand{\sfVR}{\mathsf{VR}} % Vietoris-Rips complex

% text notations
\newcommand{\ptcloudx}{\mathbb{X}} % Point cloud X
\newcommand{\ptcloudy}{\mathbb{Y}} % Point cloud Y

\newcommand{\needref}{\textbf{NEEDREF} }
\newcommand{\Frechet}{Fr\'{e}chet }
\newcommand{\Hilbert}{L_2(\mathbb{N}\times \mathbb{R})}
\newcommand{\Erdos}{Erd\H{o}s}
\newcommand{\Renyi}{R\'{e}nyi }
\newcommand{\tr}{\textrm{tr}}

\newcommand{\rmE}{\textrm{E}}
\newcommand{\rmVar}{\textrm{Var}}

% composite commands
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\future}[1]{{\color{blue}\bf (#1)}}

% collaborative works
\newcommand{\kisung}[1]{{\color{red}#1}}

% algorithm
\usepackage{algorithm,algpseudocode}% http://ctan.org/pkg/{algorithms,algorithmx}
\algnewcommand{\Inputs}[1]{%
	\State \textbf{Inputs:}
	\Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\algnewcommand{\Initialize}[1]{%
	\State \textbf{Initialize:}
	\Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}





% ***************************************************************

\parindent=15pt
\textheight 22cm \textwidth  16.5cm \oddsidemargin 0mm \topmargin     5mm
\headheight    0mm


\begin{document}
	
\title{A Note-So-Comprehensive List of \\Dissimilarity Measures for Probability Distributions}
\author[1]{Kisung You\\
	\texttt{kyoustat@gmail.com}}
%\author[2]{Lizhen Lin}
%\affil[1,2]{Department of ACMS, University of Notre Dame}
\date{\today}

\maketitle
\tableofcontents
%\begin{abstract}

%\end{abstract}

%Key words: minor computational journal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The List}\label{sec:main}

We begin this section by introducing notations. Let $(\mathcal{X},\mathcal{F}, \mu)$ be a measure space with sample space $\cal{X}$, its $\sigma$-algebra $\cal{F}$, and some Lebesgue or counting measure $\mu$. We mostly consider two probability measures $P$ and $Q$, both of which are dominated by $\mu$ with respect to Radon-Nikodym densities $p = dP/d\mu$ and $q = dQ / d\mu$. 

\begin{center}
\renewcommand{\arraystretch}{1.6} % row height
\begin{longtable}[ht]{|c|c|M|M|}
	\hline
	\rowcolor{lime} \textbf{Abbr.} & \textbf{Full Name} & \multicolumn{2}{c|}{\textbf{Definition}}\\
	\hline
	\hline
	BD & \hyperref[diss:BD]{Bhattacharyya Distance} & 
D_{BD}[P:Q] & -\log \left(\int_{\cal{X}} \sqrt{p(x) q(x)} d\mu(x)\right) \\ \hline

CSD & \hyperref[diss:CSD]{Cauchy-Schwarz Divergence} & 

D_{CSD}[P,Q] & -\log \left( \frac{\int_{\cal{X}} p(x) q(x) d\mu(x)}{
\sqrt{
\int_{\cal{X}} p(x)^2 d\mu(x) \int_{\cal{X}} q(x)^2 d\mu(x)
}
}\right)

\\ \hline
HD & \hyperref[diss:HD]{Hellinger Distance} & 
D_{HD}[P:Q] & \sqrt{\frac{1}{2} \int_{\cal{X}} \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 d\mu(x)} \\ \hline
	KLD & \hyperref[diss:KLD]{Kullback-Leibler Divergence}
& D_{KLD}[P:Q] & \int_{\cal{X}} p(x) \log \frac{p(x)}{q(x)} d\mu(x)\\ \hline

JD & \hyperref[diss:JD]{Jeffreys Divergence} &
D_{JD}[P:Q] & D_{KL}[P:Q] + D_{KL}[Q:P] \\ \hline

RD & \hyperref[diss:RD]{\Renyi Divergence} & D_{RD,\alpha}[P:Q] & \frac{1}{\alpha-1} \log \left( \int_{\cal{X}} p(x)^\alpha q(x)^{1-\alpha} d\mu(x)\right)\\
	\hline
	\caption{Summary table of dissimilarity measures.}
	\label{table:longtable}
\end{longtable}	
\end{center}


%---------------------------------------------------
\subsection{Bhattacharyya Distance}\label{diss:BD}
\cite{bhattacharyya_measure_1946} proposed a dissimilarity measure between two multinomial populations, which was later generalised for arbitrary measures. Bhattacharyya distance (BD) is closely related to the Bhattacharyya coefficient $\rho(P,Q)$ which measures the amount of overlap between two populations 
\begin{equation*}
\rho(P,Q) = \int_{\cal{X}} \sqrt{p(x) q(x)} d\mu(x)
\end{equation*}
in that the BD is defined using the coefficient

\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:BD_equation}
	D_{BD}[P:Q] = -\log (\rho(P,Q)) = -\log \left(\int_{\cal{X}} \sqrt{p(x) q(x)} d\mu(x)\right)
	\end{equation}}		
\end{center}


\begin{Properties}
	\item $D_{BD}[P:Q]$ is non-negative and symmetric. 
	\item $0 \leq \rho(P,Q) \leq 1$ so that $D_{BD} \in [0,\infty)$.
\end{Properties}
\begin{proof}[Proof of Property 2.]
	The Cauchy-Schwarz inequality for two densities gives that 
	\begin{align*}
	\rho(P,Q) &= \int_{\cal{X}} \sqrt{p(x) q(x)} d\mu(x) = |\langle \sqrt{p(x)}, \sqrt{q(x)} \rangle| \leq  \| \sqrt{p(x)}\| \| \sqrt{q(x)}\|\\
	&= \sqrt{\int_{\cal{X}} p(x) d\mu(x)} = \sqrt{\int_{\cal{X}} q(x) d\mu(x)} = 1 \cdot 1 = 1
	\end{align*}
	and taking the negative of the log of the Bhattacharyya coefficient gives the range. 
\end{proof}

%---------------------------------------------------
\subsection{Cauchy-Schwarz Divergence}\label{diss:CSD}

The Cauchy-Schwarz inequality states that for vectors $u$ and $v$ of an inner product space,
\begin{equation*}
\vert\langle u, v \rangle\vert^2 \leq \langle u,u \rangle \cdot \langle v, v \rangle
\end{equation*}
which motivates a dissimilarity measure as follows,
\begin{gather*}
\vert\langle u, v \rangle\vert  \leq \sqrt{\langle u,u \rangle} \cdot \sqrt{\langle v, v \rangle} \Rightarrow  \frac{\vert\langle u, v \rangle\vert}{ \|u\| \cdot  \|v\|} \leq 1 
\Rightarrow -\log \left( \frac{\vert\langle u, v \rangle\vert}{ \|u\| \cdot  \|v\|}  \right) \geq 0.
\end{gather*}
From the observation above, \cite{kampa_closed-form_2011-1} proposed Cauchy-Schwarz Divergence (CSD) 


\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:CSD_equation}
D_{CSD} [P_1 : P_2] = -\log \left( 
\frac{
	\int p_1 (x) p_2 (x) d\mu(x)
}{
	\sqrt{
		\int p_1(x)^2 d\mu(x) \int p_2 (x)^2 d\mu(x)
	}
}
\right) 
		\end{equation}}		
\end{center}

%---------------------------------------------------
\subsection{Hellinger Distance}\label{diss:HD}

Hellinger distance (HD) is a metric for probability distributions \citep{hellinger_neue_1909-1}. It is closely related to the Bhattacharyya distance since HD can be defined using Bhattacharyya coefficient shown in Equation \eqref{diss:BD_equation}.


\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:HD_equation}
	D_{HD}[P:Q] = \sqrt{1-\rho(P,Q)} =\left( 1 - \int_{\cal{X}} \sqrt{p(x) q(x)} d\mu(x)\right)^{1/2}
		\end{equation}}		
\end{center}


where the Equation \eqref{diss:HD_equation} is a bit different from Table \ref{table:longtable} but two are equivalent expressions, which can be derived from simple algebra.
\begin{align*}
D_{HD}^2 &= \frac{1}{2} \int_{\cal{X}} \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 d\mu(x) = \frac{1}{2} \int_{\cal{X}} \lbrace p(x) + q(x) \rbrace d\mu(x) + - \frac{2}{2} \int_{\cal{X}} \sqrt{p(x)q(x)} d\mu(x)\\
&= \frac{1}{2} + \frac{1}{2} - \int_{\cal{X}} \sqrt{p(x)q(x)} d\mu(x) = 1 - \int_{\cal{X}} \sqrt{p(x)q(x)} d\mu(x).
\end{align*}




\begin{Properties}
	\item $D_{HD}[P:Q]$ is a metric. 
\end{Properties}


%---------------------------------------------------
\subsection{Jeffreys Divergence}\label{diss:JD}

\cite{jeffreys_invariant_1946-1} proposed a divergence measure that symmetrizes the Kullback-Leibler divergence

\begin{center}
	\mybox[colback=cyan!20,top=-1mm]{\begin{equation}\label{diss:JD_equation}
	D_{JD}[P:Q] = D_{KL}[P:Q] + D_{KL}[Q:P]
	\end{equation}}
\end{center}
by summing two KL divergences of opposite directions.

%---------------------------------------------------
\subsection{Kullback-Leibler Divergence}\label{diss:KLD}


Kullback-Leibler divergence (KLD), also called relative entropy, is one of the most fundamental measure of discrepancy between two probability measures with long history since its inception by
\cite{kullback_information_1951-1}. For two measures $P$ and $Q$, KLD is defined as


\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:KLD_equation}
D_{KLD}[P:Q] = \int_{\cal{X}} p(x) \log \frac{p(x)}{q(x)} d\mu(x)
		\end{equation}}		
\end{center}



\begin{Properties}
	\item $D_{KLD}[P:Q]$ is non-negative and asymmetric.
\end{Properties}



%---------------------------------------------------
\subsection{\Renyi Divergence}\label{diss:RD}

\Renyi divergence (RD) can be considered as a generalization of several dissimilarities \citep{renyi_measures_1961}. RD involves a single parameter $\alpha \in (0,\infty),~\alpha\neq1$ known as an order that controls balance between two distributions is defined as


\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:RD_equation}
		D_{RD,\alpha}[P:Q] = \frac{1}{\alpha-1} \log \left( \int_{\cal{X}} p(x)^\alpha q(x)^{1-\alpha} d\mu(x)\right).
		\end{equation}}		
\end{center}



Some special cases for \Renyi divergence of order $\alpha$ in the limit sense include (1) twice the Bhattacharyya distance ($\alpha=1/2$) and (2) the Kullback-Leibler divergence ($\alpha=1$).

%---------------------------------------------------
\subsection{Wasserstein Distance}\label{diss:WD}

Wasserstein Distance (WD) uses the language from the theory of optimal transport \needref{d}, which is a distance over the set of measures with the finite moment of order $p$. Usually noted as $W_p$, the $p$-Wasserstein distance for two measures $P$ and $Q$ on a metric space $(\bbX, d)$ is defined as

\begin{center}
	\mybox[colback=cyan!20]{\begin{equation}\label{diss:WD_equation}
		D_{WD,p}[P:Q] = \left( \underset{\pi \in \Pi(P,Q)}{\inf} d(x,y)^p d\pi(x,y) \right)^{\frac{1}{p}}
		\end{equation}}		
\end{center}

where .. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case Study : Gaussian Distributions}\label{sec:gaussian}

For a $d$-dimensional random variable $X$, we say it is normally distributed $X \sim \calN(\mu,\Sigma)$ with two parameters $\mu \in \bbR^d$ and $\Sigma \in \bbR^{d\times d}$ for mean and variance respectively. The density function is written as
\begin{equation}\label{def:gaussian}
f(x\vert \mu, \Sigma) = (2\pi)^{-d/2} \det (\Sigma)^{-1/2} \exp \left( - \frac{(x-\mu)^\top \Sigma^{-1} (x-\mu)}{2}\right)
\end{equation}
where $\det(\Sigma)$ is the determinant of a given square matrix $\Sigma$. In this section, we derive some closed-form formula of dissimilarities introduced in Section \ref{sec:main} for a pair of Gaussian distributions $P_1 = \calN(\mu_1,\Sigma_1)$ and $P_2 = \calN(\mu_2, \Sigma_2)$ whose densities are denoted as $p_1(x)$ and $p_2(x)$ respectively. 

\subsection*{Bhattacharyya Distance}

\begin{center}
	\mybox[colback=orange!10]{\begin{equation}\label{gauss:BD}
		D_{BD} [P_1 : P_2] = \frac{1}{8} (\mu_1 - \mu_2)^\top \Sigma_*^{-1} (\mu_1 - \mu_2) + \frac{1}{2}\log\left(
		\frac{|\Sigma_*|}{\sqrt{|\Sigma_1||\Sigma_2|}}
		\right)
		\end{equation}}
\end{center}
where $\Sigma_* = (\Sigma_1 + \Sigma_2)/2$.

\begin{proof}
	We utilize the fact that \Renyi divergence of order $\alpha=1/2$ is twice the Bhattacharyya distance.
	\begin{align*}
	D_{RD,1/2}[P_1:P_2] &= -2 \log\left( \int_{\cal{X}} \sqrt{p_1(x)}\sqrt{p_2(x)} d\mu(x) \right) \\&= 2 \left\lbrace -\log\left( \int_{\cal{X}} \sqrt{p_1(x)p_2(x)} d\mu(x)\right) \right\rbrace= 2 D_{BD}[P_1:P_2].
	\end{align*}
	By plugging $\alpha=1/2$  in the Equation \eqref{gauss:RD}, we get
	\begin{equation*}
	D_{RD,1/2}[P_1:P_2] = \frac{1}{4} \Delta \mu^\top \left\lbrack
	\frac{\Sigma_1+\Sigma_2}{2}
	\right\rbrack^{-1} \Delta \mu + \log \left(
	\frac{
		|(\Sigma_1+\Sigma_2)/2|
	}{
	|\Sigma_1|^{1/2} |\Sigma_2|^{1/2}
	}
	\right)
	\end{equation*}
	where $\Delta \mu = \mu_1 - \mu_2$. Dividing the above by 2, we acquire the result as shown in Equation \eqref{gauss:BD}.
\end{proof}


\subsection*{Cauchy-Schwarz Divergence}

\begin{center}
	\mybox[colback=orange!10]{\begin{equation}\label{gauss:CSD}
	D_{CSD} [P_1 : P_2] = -\log\left(
\frac{\calN(\mu_1\vert \mu_2, \Sigma_1+\Sigma_2)}{\sqrt{\calN(\mu_1\vert \mu_1, 2\Sigma_1)\cdot \calN(\mu_2\vert \mu_2, 2\Sigma_2)}}
\right)
		\end{equation}}		
\end{center}

\begin{proof}
We use \ref{fact:gaussian_product} repeatedly where 
\begin{align*}
\int p_1 (x) p_2 (x) d\mu(x) &= \calN(\mu_1 \vert \mu_2, \Sigma_1 + \Sigma_2) \int_{\cal{X}} \calN(x|\mu_{12},\Sigma_{12}) d\mu(x)= \calN(\mu_1\vert\mu_2, \Sigma_1+\Sigma_2)\\
\int p_i (x)^2 d\mu(x) &= \calN(\mu_i\vert \mu_i, \Sigma_i + \Sigma_i) \int_{\cal{X}} \calN(x\vert \mu_{ii},\Sigma_{ii}) d\mu(x) = \calN(\mu_i \vert \mu_i, 2\Sigma_i)\textrm{ for }i=1,2
\end{align*}
and plugging the above in the definition gives the closed-form expression in Equation \eqref{gauss:CSD}. 
\end{proof}

\subsection*{Hellinger Distance}
\begin{center}
	\mybox[colback=orange!10,top=-0.2mm]{\begin{equation}\label{gauss:HD}
		D_{HD} [P_1 : P_2] = \left[
		1 - 
		\frac{(|\Sigma_1||\Sigma_2|)^{1/4}}{|\Sigma_*|^{1/2}}		
		\cdot\exp\left(-\frac{1}{8}
(\mu_1 - \mu_2)^\top \Sigma_*^{-1}  (\mu_1-\mu_2)
		\right)
		\right]^{1/2}
		\end{equation}}		
\end{center}
where $\Sigma_* = (\Sigma_1+\Sigma_2)/2$. 
\begin{proof}
	We use the following relation with respect to the Bhattacharyya coefficient ,
	\begin{align*}
	D_{BD}[P_1:P_2] = -\log \rho(P_1,P_2) \quad&\leftrightarrow\quad \rho(P_1,P_2) = \exp(-D_{BD}[P_1:P_2])\\
	D_{HD}[P_1:P_2] = \sqrt{1-\rho(P_1,P_2)}\quad&\leftrightarrow\quad
	\rho(P_1,P_2) = 1 - 	D_{HD}[P_1:P_2]^2
	\end{align*}
	so that we have
	\begin{equation*}
	\exp(-D_{BD}[P_1:P_2])	= 1 - 	D_{HD}[P_1:P_2]^2 ~\rightarrow~ D_{HD}[P_1:P_2] = \sqrt{1-\exp(-D_{BD}[P_1:P_2])}.
	\end{equation*}
	Since we are given close-form formulae of the Bhattacharyya distance in Equation \eqref{gauss:BD}, re-arranging the terms with respect to the above relation gives the result. 
\end{proof}


\subsection*{\Renyi Divergence}
We assume the order $\alpha \neq 1$ since the Equation \eqref{diss:RD_equation} is not properly defined and the equivalence to KL divergence makes sense only in the limiting sense.

% definitions are a bit different.
% http://web.eecs.umich.edu/~hero/Preprints/cspl-328.pdf
\begin{center}
		\mybigbox[colback=orange!10, top=-1mm]{
			\begin{equation}\label{gauss:RD}
			\begin{split}
			D_{RD,\alpha} [P_1 : P_2] =& \frac{\alpha}{2} (\mu_1 - \mu_2)^\top \lbrack 
			\alpha \Sigma_2 + (1-\alpha)\Sigma_1
			\rbrack^{-1}(\mu_1 - \mu_2)\\
			& - \frac{1}{2(\alpha-1)} \log \left(
			\frac{\vert \alpha \Sigma_2 + (1-\alpha)\Sigma_1\vert}{\vert\Sigma_1\vert^{1-\alpha} \vert \Sigma_2\vert^\alpha}
			\right)
			\end{split}
		\end{equation}}		
\end{center}

\begin{proof}
	The \Renyi divergence of order $\alpha$ for two densities $p_1$ and $p_2$ is defined as
	\begin{equation*}
	D_{RD,\alpha}[P_1:P_2] = \frac{1}{\alpha-1} \log \int p_1 (x)^\alpha p_2 (x)^{1-\alpha } d\mu(x)
	\end{equation*}
	so we focus on the integral term, 
	\begin{align*}
	\int p_1 (x)^\alpha &p_2 (x)^{1-\alpha } d\mu(x) = \int \left[ (2\pi)^{-d/2} |\Sigma_1|^{-1/2} \exp \left( -\frac{(x-\mu_1)^\top \Sigma_1^{-1} (x-\mu_1)}{2} \right) \right]^{\alpha} \\
	& \times \left[ (2\pi)^{-d/2} |\Sigma_2|^{-1/2} \exp \left( -\frac{(x-\mu_2)^\top \Sigma_2^{-1} (x-\mu_2)}{2} \right) \right]^{1-\alpha} \\
	&= (2\pi)^{-d/2} |\Sigma_1|^{-\frac{\alpha}{2}} |\Sigma_2|^{-\frac{1-\alpha}{2}}\\
	& \times
	\int\exp\left(-\frac{ (x-\mu_1)^\top \alpha \Sigma_1^{-1} (x-\mu_1) + (x-\mu_2)^\top (1-\alpha) \Sigma_2^{-1} (x-\mu_2) }{2}\right)d\mu(x).
	\end{align*}
	We can integrate out the second term by completing the square in a multivariate manner
	\begin{equation*}
	(x-\mu_1)^\top \alpha \Sigma_1^{-1} (x-\mu_1) + (x-\mu_2)^\top (1-\alpha) \Sigma_2^{-1} (x-\mu_2) = (x-\tilde{\mu})^\top S^{-1} (x-\tilde{\mu}) + C
	\end{equation*}
	where
	\begin{align*}
	S &= \left[\alpha \Sigma_1^{-1} + (1-\alpha) \Sigma_2^{-1}\right]^{-1} \\
		C &= \alpha(1-\alpha) (\mu_1-\mu_2)^\top \left[
		\alpha \Sigma_2 + (1-\alpha) \Sigma_1
		\right]^{-1} (\mu_1-\mu_2)\\
	\tilde{\mu} &= S \left( \alpha \Sigma_1^{-1} \mu_1 + (1-\alpha) \Sigma_2^{-1} \mu_2 \right).
	\end{align*}
	We denote $\Delta \mu = \mu_1 - \mu_2$ and the above simplification gives 
	\begin{align*}
	\int &p_1 (x)^\alpha p_2 (x)^{1-\alpha } d\mu(x) = (2\pi)^{-d/2} |\Sigma_1|^{-\frac{\alpha}{2}} |\Sigma_2|^{-\frac{1-\alpha}{2}} \exp\left(-\frac{1}{2}C\right) (2\pi)^{d/2} |S|^{1/2}\\
	&= \frac{| \alpha \Sigma_1^{-1} + (1-\alpha)\Sigma_2^{-1}|^{-1/2}}{|\Sigma_1|^{\alpha/2} |\Sigma_2|^{(1-\alpha)/2}} \exp\left(-\frac{\alpha(1-\alpha) \Delta \mu^\top \left[
		\alpha \Sigma_2 + (1-\alpha) \Sigma_1
		\right]^{-1} \Delta \mu}{2} \right)\\
	\intertext{and multiply $(|\Sigma_1||\Sigma_2|)^{-1/2}$ in the denominator and numerator of the first term using the fact that $|AB|=|A||B|$ naturally implies $|AB|^{-1/2}=(|A||B|)^{-1/2}$ so that}
	&= \frac{
|\alpha \Sigma_2 + (1-\alpha)\Sigma_1|^{-1/2}	
}{|\Sigma_1|^{\frac{\alpha-1}{2}} |\Sigma_2|^{-\frac{\alpha}{2}} }\exp\left(\frac{\alpha(\alpha-1) \Delta \mu^\top \left[
		\alpha \Sigma_2 + (1-\alpha) \Sigma_1
		\right]^{-1} \Delta \mu}{2} \right)
	\intertext{so that finally we reach the following;}
	D_{RD,\alpha}[P_1:P_2] &= 
	\frac{\alpha(\alpha-1) \Delta \mu^\top \left[
		\alpha \Sigma_2 + (1-\alpha) \Sigma_1
		\right]^{-1} \Delta \mu}{2(\alpha-1)} + \frac{1}{\alpha-1} \log \left( \frac{
		|\alpha \Sigma_2 + (1-\alpha)\Sigma_1|^{-1/2}	
	}{|\Sigma_1|^{\frac{\alpha-1}{2}} |\Sigma_2|^{-\frac{\alpha}{2}} }\right) \\
&= \frac{\alpha}{2} \Delta \mu^\top \lbrack 
\alpha \Sigma_2 + (1-\alpha)\Sigma_1
\rbrack^{-1} \Delta \mu
 - \frac{1}{2(\alpha-1)} \log \left(
\frac{\vert \alpha \Sigma_2 + (1-\alpha)\Sigma_1\vert}{\vert\Sigma_1\vert^{1-\alpha} \vert \Sigma_2\vert^\alpha}
\right)
	\end{align*}
	which completes the derivation.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Miscellaneous Facts}\label{sec:misc}

In this section, we introduce some miscellaneous facts. We use $P_i = \calN(\mu_i, \Sigma_i) \textrm{ for } i \in \mathcal{I}$ for Gaussian distributions in $\bbR^d$. 

\begin{Facts}
	\item \textbf{(product of two Gaussians)}\label{fact:gaussian_product}
	
		For $P_1$ and $P_2$, product of two normal densities is a scaled normal density,
	\begin{equation*}
	p_1 (x) \cdot p_2 (x) = \calN(x\vert \mu_1, \Sigma_1) \cdot \calN(x\vert \mu_2, \Sigma_2) = c_{12}\cdot \calN(x\vert \mu_{12}, \Sigma_{12})
	\end{equation*} where
	\begin{align*}
	c_{12} &= \calN(\mu_1 \vert \mu_2, (\Sigma_1+\Sigma_2)) \\
	\Sigma_{12} &= (\Sigma_1^{-1} + \Sigma_2^{-1})^{-1} \\
	\mu_{12} &= \Sigma_{12} \cdot (\Sigma_1^{-1}\mu_1 + \Sigma_2^{-1} \mu_2) 
	\end{align*}
	according to Section 8.1.8 of \cite{petersen_matrix_2012}.
	\item \textbf{(entropy of Gaussian distribution)}\label{fact:gaussian_entropy}
	
	The differential entropy is of a random variable $X$ with density $p(x)$ is defined as 
	\begin{equation*}
	H(X) = -\int p(x) \log p(x) dx = -\bbE_p [\log p(x)]
	\end{equation*}
	and for $X \sim \calN(\mu, \Sigma)$ in $\bbR^d$, 
	\begin{equation*}
	H(X) = \frac{d}{2}\log(2\pi) + \frac{1}{2}\log \det(\Sigma) + \frac{1}{2}d
	\end{equation*}
	which can be derived as follows;
	\begin{align*}
	H(X) &= -\bbE\left[ \log \left\lbrace (2\pi)^{-d/2} \det(\Sigma)^{-1/2} \exp \left( -\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) \right)  \right\rbrace \right] \\
	&= -\bbE \left[
	-\frac{d}{2}\log (2\pi) - \frac{1}{2}\log\det(\Sigma) - \frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu)
	\right]\\
	\intertext{and we can pull the constant terms out of the expectation}
	&=\frac{d}{2}\log (2\pi) + \frac{1}{2}\log\det(\Sigma) + \frac{1}{2}\bbE\left[(x-\mu)^\top \Sigma^{-1} (x-\mu)\right].
	\end{align*}
	The last term is reduced to $d/2$ by the \textit{trace trick};
	\begin{align*}
	\bbE\left[(x-\mu)^\top \Sigma^{-1} (x-\mu)\right] &= \bbE\left[ \tr \left( (x-\mu)^\top \Sigma^{-1} (x-\mu) \right) \right] = \bbE\left[ \tr \left( (x-\mu)(x-\mu)^\top \Sigma^{-1}  \right) \right] \\
	&= \tr \left[ \bbE \left((x-\mu)(x-\mu)^\top\right) \Sigma^{-1}\right] = \tr \left( \Sigma \Sigma^{-1} \right) = \tr (I_d) = d
	\end{align*}
	in that we can acquire the following form,
	\begin{equation*}
	H(X) =  \frac{d}{2}\log (2\pi) + \frac{1}{2}\log\det(\Sigma) + \frac{1}{2}d.
	\end{equation*}
	\item \textbf{(integral of square root density)}\label{fact:gaussian_sqintegral}
	
	The Bhattacharyya coefficient involves evaluation for the integral of square root density. We derive the result with respect to a single Gaussian distribution  $P = \calN(\mu,\Sigma)$ in $\bbR^d$, then
	\begin{equation*}
		\int_{\bbR^d} \sqrt{p(x)} dx = (8\pi)^{d/4} |\Sigma|^{1/4}
	\end{equation*}
	which can be derived as follows;
	\begin{align*}
		\int_{\bbR^d} \sqrt{p(x)} dx &= \int (2\pi)^{-d/4} |\Sigma|^{-1/4} \exp \left(-\frac{1}{2} (x-\mu)^\top (2\Sigma)^{-1} (x-\mu) \right) \\
		&=(2\pi)^{-d/4} \cdot |\Sigma|^{-1/4}\cdot (2\pi)^{d/2}\cdot |2\Sigma|^{1/2} \int \cdots dx \\
		&= (2\pi)^{d/4}\cdot |\Sigma|^{-1/4} \cdot 2^{d/2} \cdot|\Sigma|^{1/2} \\
		&= \pi^{d/4} \cdot2^{3d/4} \cdot|\Sigma|^{1/4} = (8\pi)^{d/4} |\Sigma|^{1/4}
	\end{align*}
	where the integral with $\cdots$ is a Gaussian distribution with scaled variance parameter which integrates to 1. 
\end{Facts}

\bibliographystyle{dcu}
\bibliography{reference}

\end{document}
